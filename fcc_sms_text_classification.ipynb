{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Starzenpro/Masilo/blob/main/fcc_sms_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "# Load the SMS Spam Collection dataset\n",
        "# The dataset is tab-separated and has no header row\n",
        "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Split data into training and testing sets (replace with your desired split)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_messages, test_messages, train_labels, test_labels = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to numeric (0 for ham, 1 for spam)\n",
        "train_labels_numeric = train_labels.map({'ham': 0, 'spam': 1})\n",
        "test_labels_numeric = test_labels.map({'ham': 0, 'spam': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "# Create and train the model\n",
        "model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),  # Convert text to TF-IDF features\n",
        "    ('classifier', MultinomialNB())     # Naive Bayes classifier for text data\n",
        "])\n",
        "model.fit(train_messages, train_labels_numeric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [],
      "source": [
        "# Define prediction function\n",
        "def predict_message(message):\n",
        "    # Get probability prediction for the input message\n",
        "    proba = model.predict_proba([message])[0]\n",
        "    # Probability that the message is spam (class 1)\n",
        "    prob_spam = proba[1]\n",
        "    # Determine label based on probability threshold (0.5)\n",
        "    label = 'spam' if prob_spam >= 0.5 else 'ham'\n",
        "    return [prob_spam, label]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "print(predict_message(\"Hello, how are you doing today?\"))\n",
        "print(predict_message(\"WINNER!! Claim your free iPhone now!\"))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83879090"
      },
      "source": [
        "# Load a sample dataset (replace with your actual data loading)\n",
        "data = {'message': [\"Hello, how are you?\", \"WINNER! Claim your prize!\", \"Meeting at 3 PM.\", \"Free money!\", \"Good morning.\"],\n",
        "        'label': [\"ham\", \"spam\", \"ham\", \"spam\", \"ham\"]}\n",
        "df = pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21e5cf08"
      },
      "source": [
        "# Split data into training and testing sets (replace with your desired split)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_messages, test_messages, train_labels, test_labels = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3d40086"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create and train the model\n",
        "model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),  # Convert text to TF-IDF features\n",
        "    ('classifier', MultinomialNB())     # Naive Bayes classifier for text data\n",
        "])\n",
        "model.fit(train_messages, train_labels_numeric)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(test_messages)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(test_labels_numeric, predictions)\n",
        "precision = precision_score(test_labels_numeric, predictions)\n",
        "recall = recall_score(test_labels_numeric, predictions)\n",
        "f1 = f1_score(test_labels_numeric, predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "f1 = f1_score(test_labels_numeric, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e7fc5c9"
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "!unzip smsspamcollection.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e08a47a6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "# The dataset is tab-separated and has no header row\n",
        "df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Display the first few rows\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee94b3a1"
      },
      "source": [
        "# Define prediction function\n",
        "def predict_message(message):\n",
        "    # Get probability prediction for the input message\n",
        "    proba = model.predict_proba([message])[0]\n",
        "    # Probability that the message is spam (class 1)\n",
        "    prob_spam = proba[1]\n",
        "    # Determine label based on probability threshold (0.5)\n",
        "    label = 'spam' if prob_spam >= 0.5 else 'ham'\n",
        "    return [prob_spam, label]\n",
        "\n",
        "# Test the predict_message function with custom messages\n",
        "\n",
        "# Example 1: A likely ham message\n",
        "message1 = \"Just wanted to confirm our meeting for tomorrow at 10 AM.\"\n",
        "prediction1 = predict_message(message1)\n",
        "print(f\"Message: '{message1}'\")\n",
        "print(f\"Prediction: Probability of spam = {prediction1[0]:.4f}, Label = {prediction1[1]}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Example 2: A likely spam message\n",
        "message2 = \"URGENT! You have won a prize! Click here to claim now!\"\n",
        "prediction2 = predict_message(message2)\n",
        "print(f\"Message: '{message2}'\")\n",
        "print(f\"Prediction: Probability of spam = {prediction2[0]:.4f}, Label = {prediction2[1]}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Example 3: Another custom message\n",
        "message3 = \"Can you pick up some groceries on your way home?\"\n",
        "prediction3 = predict_message(message3)\n",
        "print(f\"Message: '{message3}'\")\n",
        "print(f\"Prediction: Probability of spam = {prediction3[0]:.4f}, Label = {prediction3[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d90c0805"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(test_labels_numeric, predictions)\n",
        "\n",
        "# Display the confusion matrix using seaborn and matplotlib\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdff9c48"
      },
      "source": [
        "# Task\n",
        "Improve the performance of the spam classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89d93709"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Explore techniques like text cleaning (removing punctuation, stop words, etc.), stemming, and lemmatization to improve the quality of the text data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07aca865"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for text preprocessing and define a function to clean the text data, then apply the function to the training and testing messages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bef6b0b6"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "try:\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# Define a text cleaning function\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove stop words\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
        "    # Apply stemming or lemmatization (choose one)\n",
        "    # text = ' '.join([stemmer.stem(word) for word in text.split()]) # Stemming\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()]) # Lemmatization\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the training and testing messages\n",
        "train_messages_cleaned = train_messages.apply(clean_text)\n",
        "test_messages_cleaned = test_messages.apply(clean_text)\n",
        "\n",
        "# Display the first few cleaned messages from training set\n",
        "print(\"Original training messages:\")\n",
        "print(train_messages.head())\n",
        "print(\"\\nCleaned training messages:\")\n",
        "print(train_messages_cleaned.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dc35611"
      },
      "source": [
        "**Reasoning**:\n",
        "The text cleaning step is complete. The next step is to update the model training and evaluation to use the cleaned text data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEPY0vDWu52B"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create and train the model with cleaned data\n",
        "model_cleaned = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),  # Convert text to TF-IDF features\n",
        "    ('classifier', MultinomialNB())     # Naive Bayes classifier for text data\n",
        "])\n",
        "model_cleaned.fit(train_messages_cleaned, train_labels_numeric)\n",
        "\n",
        "# Make predictions on the cleaned test set\n",
        "predictions_cleaned = model_cleaned.predict(test_messages_cleaned)\n",
        "\n",
        "# Calculate evaluation metrics for the model trained on cleaned data\n",
        "accuracy_cleaned = accuracy_score(test_labels_numeric, predictions_cleaned)\n",
        "precision_cleaned = precision_score(test_labels_numeric, predictions_cleaned)\n",
        "recall_cleaned = recall_score(test_labels_numeric, predictions_cleaned)\n",
        "f1_cleaned = f1_score(test_labels_numeric, predictions_cleaned)\n",
        "\n",
        "# Print the results\n",
        "print(\"Metrics with cleaned data:\")\n",
        "print(f\"Accuracy: {accuracy_cleaned:.2f}\")\n",
        "print(f\"Precision: {precision_cleaned:.2f}\")\n",
        "print(f\"Recall: {recall_cleaned:.2f}\")\n",
        "print(f\"F1 Score: {f1_cleaned:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aa17ec4"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "### Subtask:\n",
        "Investigate using different text vectorization methods (e.g., CountVectorizer, or more advanced techniques like word embeddings) or incorporating other relevant features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66894db8"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires creating a new pipeline with CountVectorizer and MultinomialNB, training it on cleaned data, making predictions, and evaluating the performance. This can be done in a single code block by following the steps outlined in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f2afd50"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a new Pipeline with CountVectorizer and MultinomialNB\n",
        "model_countvectorizer = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),  # Convert text to CountVectorizer features\n",
        "    ('classifier', MultinomialNB())     # Naive Bayes classifier for text data\n",
        "])\n",
        "\n",
        "# Train the model on the cleaned training messages and numeric labels\n",
        "model_countvectorizer.fit(train_messages_cleaned, train_labels_numeric)\n",
        "\n",
        "# Make predictions on the cleaned test messages\n",
        "predictions_countvectorizer = model_countvectorizer.predict(test_messages_cleaned)\n",
        "\n",
        "# Calculate evaluation metrics for the model trained with CountVectorizer\n",
        "accuracy_countvectorizer = accuracy_score(test_labels_numeric, predictions_countvectorizer)\n",
        "precision_countvectorizer = precision_score(test_labels_numeric, predictions_countvectorizer)\n",
        "recall_countvectorizer = recall_score(test_labels_numeric, predictions_countvectorizer)\n",
        "f1_countvectorizer = f1_score(test_labels_numeric, predictions_countvectorizer)\n",
        "\n",
        "# Print the results\n",
        "print(\"Metrics with CountVectorizer:\")\n",
        "print(f\"Accuracy: {accuracy_countvectorizer:.2f}\")\n",
        "print(f\"Precision: {precision_countvectorizer:.2f}\")\n",
        "print(f\"Recall: {recall_countvectorizer:.2f}\")\n",
        "print(f\"F1 Score: {f1_countvectorizer:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "571e5ee7"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "### Subtask:\n",
        "Experiment with different classification algorithms (e.g., Support Vector Machines, Logistic Regression, or deep learning models) to see if they perform better than Naive Bayes for this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dccc61b4"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classification algorithms and create and train the new pipelines using CountVectorizer. Then make predictions and calculate the evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14bdbc48"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Create Logistic Regression pipeline with CountVectorizer\n",
        "logreg_pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Create SVC pipeline with CountVectorizer\n",
        "svc_pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', SVC(probability=True)) # probability=True is needed for predict_proba later if needed\n",
        "])\n",
        "\n",
        "# Train the Logistic Regression pipeline\n",
        "logreg_pipeline.fit(train_messages_cleaned, train_labels_numeric)\n",
        "\n",
        "# Train the SVC pipeline\n",
        "svc_pipeline.fit(train_messages_cleaned, train_labels_numeric)\n",
        "\n",
        "# Make predictions with Logistic Regression\n",
        "logreg_predictions = logreg_pipeline.predict(test_messages_cleaned)\n",
        "\n",
        "# Make predictions with SVC\n",
        "svc_predictions = svc_pipeline.predict(test_messages_cleaned)\n",
        "\n",
        "# Calculate and print metrics for Logistic Regression\n",
        "logreg_accuracy = accuracy_score(test_labels_numeric, logreg_predictions)\n",
        "logreg_precision = precision_score(test_labels_numeric, logreg_predictions)\n",
        "logreg_recall = recall_score(test_labels_numeric, logreg_predictions)\n",
        "logreg_f1 = f1_score(test_labels_numeric, logreg_predictions)\n",
        "\n",
        "print(\"Metrics for Logistic Regression with CountVectorizer:\")\n",
        "print(f\"Accuracy: {logreg_accuracy:.2f}\")\n",
        "print(f\"Precision: {logreg_precision:.2f}\")\n",
        "print(f\"Recall: {logreg_recall:.2f}\")\n",
        "print(f\"F1 Score: {logreg_f1:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Calculate and print metrics for SVC\n",
        "svc_accuracy = accuracy_score(test_labels_numeric, svc_predictions)\n",
        "svc_precision = precision_score(test_labels_numeric, svc_predictions)\n",
        "svc_recall = recall_score(test_labels_numeric, svc_predictions)\n",
        "svc_f1 = f1_score(test_labels_numeric, svc_predictions)\n",
        "\n",
        "print(\"Metrics for SVC with CountVectorizer:\")\n",
        "print(f\"Accuracy: {svc_accuracy:.2f}\")\n",
        "print(f\"Precision: {svc_precision:.2f}\")\n",
        "print(f\"Recall: {svc_recall:.2f}\")\n",
        "print(f\"F1 Score: {svc_f1:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02198339"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Optimize the hyperparameters of the chosen model and the vectorizer using techniques like Grid Search or Random Search to find the best configuration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b084c89e"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary modules and define the parameter grid for GridSearchCV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce7fa0ce"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define a pipeline for Logistic Regression with CountVectorizer\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', max_iter=1000)) # Use 'liblinear' for smaller datasets and add max_iter\n",
        "])\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'vectorizer__max_features': [1000, 2000, 3000],\n",
        "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
        "    'classifier__C': [0.1, 1, 10]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f38b055"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate and fit GridSearchCV to find the best parameters and evaluate the best model on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7b221d6"
      },
      "source": [
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=['accuracy', 'precision', 'recall', 'f1'], refit='f1')\n",
        "\n",
        "# Fit GridSearchCV to the cleaned training data\n",
        "grid_search.fit(train_messages_cleaned, train_labels_numeric)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "print(\"\\nBest cross-validation F1 score:\")\n",
        "print(f\"{grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Evaluate the best model on the cleaned test data\n",
        "best_model = grid_search.best_estimator_\n",
        "test_predictions = best_model.predict(test_messages_cleaned)\n",
        "\n",
        "# Calculate and print evaluation metrics on the test set\n",
        "test_accuracy = accuracy_score(test_labels_numeric, test_predictions)\n",
        "test_precision = precision_score(test_labels_numeric, test_predictions)\n",
        "test_recall = recall_score(test_labels_numeric, test_predictions)\n",
        "test_f1 = f1_score(test_labels_numeric, test_predictions)\n",
        "\n",
        "print(\"\\nMetrics on the cleaned test set with the best model:\")\n",
        "print(f\"Accuracy: {test_accuracy:.2f}\")\n",
        "print(f\"Precision: {test_precision:.2f}\")\n",
        "print(f\"Recall: {test_recall:.2f}\")\n",
        "print(f\"F1 Score: {test_f1:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c99c19d"
      },
      "source": [
        "## Cross-validation\n",
        "\n",
        "### Subtask:\n",
        "Implement cross-validation to get a more robust estimate of the model's performance and ensure it generalizes well to unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47689fcd"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement cross-validation using the best model found in the previous step to get a more robust performance estimate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31861534"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Use cross_val_score with the best model and cleaned training data\n",
        "cv_scores = cross_val_score(best_model, train_messages_cleaned, train_labels_numeric, cv=5, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
        "\n",
        "# Print the mean and standard deviation of the cross-validation scores\n",
        "print(\"Cross-validation scores:\")\n",
        "print(f\"Mean Accuracy: {cv_scores['test_accuracy'].mean():.2f} (+/- {cv_scores['test_accuracy'].std():.2f})\")\n",
        "print(f\"Mean Precision: {cv_scores['test_precision'].mean():.2f} (+/- {cv_scores['test_precision'].std():.2f})\")\n",
        "print(f\"Mean Recall: {cv_scores['test_recall'].mean():.2f} (+/- {cv_scores['test_recall'].std():.2f})\")\n",
        "print(f\"Mean F1 Score: {cv_scores['test_f1'].mean():.2f} (+/- {cv_scores['test_f1'].std():.2f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ba4c03c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `cross_val_score`'s `scoring` parameter expects a single string or a callable, not a list of strings. I need to use `cross_validate` instead to get multiple scoring metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6610F5q_vOXd"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# Use cross_validate with the best model and cleaned training data\n",
        "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "cv_results = cross_validate(best_model, train_messages_cleaned, train_labels_numeric, cv=5, scoring=scoring)\n",
        "\n",
        "# Print the mean and standard deviation of the cross-validation scores\n",
        "print(\"Cross-validation results:\")\n",
        "for metric in scoring:\n",
        "    mean_score = cv_results[f'test_{metric}'].mean()\n",
        "    std_score = cv_results[f'test_{metric}'].std()\n",
        "    print(f\"Mean {metric.capitalize()}: {mean_score:.2f} (+/- {std_score:.2f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57c42767"
      },
      "source": [
        "## Error analysis\n",
        "\n",
        "### Subtask:\n",
        "Analyze the instances where the model makes incorrect predictions (false positives and false negatives) to gain insights into the model's weaknesses and identify areas for improvement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "518f46ef"
      },
      "source": [
        "**Reasoning**:\n",
        "Get predictions from the best model, create a DataFrame with original and predicted labels, and then filter for false positives and false negatives to analyze misclassifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e77b4a47"
      },
      "source": [
        "# 1. Get the predicted labels from the best_model on the test_messages_cleaned\n",
        "test_predictions = best_model.predict(test_messages_cleaned)\n",
        "\n",
        "# 2. Create a DataFrame containing the original test_messages, test_labels, and the test_predictions\n",
        "results_df = pd.DataFrame({\n",
        "    'original_message': test_messages,\n",
        "    'true_label': test_labels,\n",
        "    'predicted_label': test_predictions\n",
        "})\n",
        "\n",
        "# Convert predicted_label back to 'ham'/'spam' for easier comparison with true_label\n",
        "results_df['predicted_label_str'] = results_df['predicted_label'].map({0: 'ham', 1: 'spam'})\n",
        "\n",
        "# 3. Filter this DataFrame to identify false positives (where the true label is 'ham' but the prediction is 'spam')\n",
        "false_positives = results_df[(results_df['true_label'] == 'ham') & (results_df['predicted_label_str'] == 'spam')]\n",
        "\n",
        "# 4. Filter the DataFrame to identify false negatives (where the true label is 'spam' but the prediction is 'ham')\n",
        "false_negatives = results_df[(results_df['true_label'] == 'spam') & (results_df['predicted_label_str'] == 'ham')]\n",
        "\n",
        "# 5. Display the false positives and false negatives DataFrames\n",
        "print(\"False Positives:\")\n",
        "display(false_positives)\n",
        "\n",
        "print(\"\\nFalse Negatives:\")\n",
        "display(false_negatives)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06bb40cd"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Text cleaning techniques (lowercase conversion, punctuation removal, stop word removal, and lemmatization) were applied, resulting in a model trained on cleaned data achieving an accuracy of 0.97, precision of 1.00, recall of 0.78, and F1 score of 0.88.\n",
        "*   Using `CountVectorizer` for text vectorization improved the model's performance compared to `TfidfVectorizer`, with metrics on cleaned data showing an accuracy of 0.99, precision of 0.97, recall of 0.92, and F1 score of 0.94.\n",
        "*   Exploring different classification algorithms showed that both Logistic Regression and SVC achieved high accuracy (0.98) and precision (1.00) when used with `CountVectorizer`. Logistic Regression slightly outperformed SVC with a recall of 0.89 and an F1 score of 0.94.\n",
        "*   Hyperparameter tuning using `GridSearchCV` with Logistic Regression and `CountVectorizer` identified the best parameters as `{'classifier__C': 10, 'vectorizer__max_features': 3000, 'vectorizer__ngram_range': (1, 2)}`. The model with these parameters achieved a cross-validation F1 score of 0.91 and test set metrics of Accuracy: 0.99, Precision: 0.99, Recall: 0.90, F1 Score: 0.94.\n",
        "*   Cross-validation results using the best model showed consistent performance across 5 folds, with mean scores and standard deviations reported for accuracy, precision, recall, and F1.\n",
        "*   Error analysis revealed a low number of false positives (only one instance) but a notable number of false negatives, indicating the model struggles to identify certain types of spam messages.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Focus on techniques to improve the model's ability to detect diverse spam patterns to reduce false negatives, potentially through advanced feature engineering or exploring more sophisticated models.\n",
        "*   Investigate the specific characteristics of the messages misclassified as false negatives to identify common themes or features that the current model is missing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e148db9"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Explore techniques like text cleaning (removing punctuation, stop words, etc.), stemming, and lemmatization to improve the quality of the text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36fb6d72"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for text preprocessing and define a function to clean the text data, then apply the function to the training and testing messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "476186a7"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "try:\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# Define a text cleaning function\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove stop words\n",
        "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
        "    # Apply stemming or lemmatization (choose one)\n",
        "    # text = ' '.join([stemmer.stem(word) for word in text.split()]) # Stemming\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()]) # Lemmatization\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the training and testing messages\n",
        "train_messages_cleaned = train_messages.apply(clean_text)\n",
        "test_messages_cleaned = test_messages.apply(clean_text)\n",
        "\n",
        "# Display the first few cleaned messages from training set\n",
        "print(\"Original training messages:\")\n",
        "print(train_messages.head())\n",
        "print(\"\\nCleaned training messages:\")\n",
        "print(train_messages_cleaned.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dff51320"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires creating a new pipeline with CountVectorizer and MultinomialNB, training it on cleaned data, making predictions, and evaluating the performance. This can be done in a single code block by following the steps outlined in the instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cc27e11"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Create a new Pipeline with CountVectorizer and MultinomialNB\n",
        "model_countvectorizer = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),  # Convert text to CountVectorizer features\n",
        "    ('classifier', MultinomialNB())     # Naive Bayes classifier for text data\n",
        "])\n",
        "\n",
        "# Train the model on the cleaned training messages and numeric labels\n",
        "model_countvectorizer.fit(train_messages_cleaned, train_labels_numeric)\n",
        "\n",
        "# Make predictions on the cleaned test messages\n",
        "predictions_countvectorizer = model_countvectorizer.predict(test_messages_cleaned)\n",
        "\n",
        "# Calculate evaluation metrics for the model trained with CountVectorizer\n",
        "accuracy_countvectorizer = accuracy_score(test_labels_numeric, predictions_countvectorizer)\n",
        "precision_countvectorizer = precision_score(test_labels_numeric, predictions_countvectorizer)\n",
        "recall_countvectorizer = recall_score(test_labels_numeric, predictions_countvectorizer)\n",
        "f1_countvectorizer = f1_score(test_labels_numeric, predictions_countvectorizer)\n",
        "\n",
        "# Print the results\n",
        "print(\"Metrics with CountVectorizer:\")\n",
        "print(f\"Accuracy: {accuracy_countvectorizer:.2f}\")\n",
        "print(f\"Precision: {precision_countvectorizer:.2f}\")\n",
        "print(f\"Recall: {recall_countvectorizer:.2f}\")\n",
        "print(f\"F1 Score: {f1_countvectorizer:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dce22a4"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classification algorithms and create and train the new pipelines using CountVectorizer. Then make predictions and calculate the evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dfee86e"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Create Logistic Regression pipeline with CountVectorizer\n",
        "logreg_pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Create SVC pipeline with CountVectorizer\n",
        "svc_pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', SVC(probability=True)) # probability=True is needed for predict_proba later if needed\n",
        "])\n",
        "\n",
        "# Train the Logistic Regression pipeline\n",
        "logreg_pipeline.fit(train_messages_cleaned, train_labels_numeric)\n",
        "\n",
        "# Train the SVC pipeline\n",
        "svc_pipeline.fit(train_messages_cleaned, train_labels_numeric)\n",
        "\n",
        "# Make predictions with Logistic Regression\n",
        "logreg_predictions = logreg_pipeline.predict(test_messages_cleaned)\n",
        "\n",
        "# Make predictions with SVC\n",
        "svc_predictions = svc_pipeline.predict(test_messages_cleaned)\n",
        "\n",
        "# Calculate and print metrics for Logistic Regression\n",
        "logreg_accuracy = accuracy_score(test_labels_numeric, logreg_predictions)\n",
        "logreg_precision = precision_score(test_labels_numeric, logreg_predictions)\n",
        "logreg_recall = recall_score(test_labels_numeric, logreg_predictions)\n",
        "logreg_f1 = f1_score(test_labels_numeric, logreg_predictions)\n",
        "\n",
        "print(\"Metrics for Logistic Regression with CountVectorizer:\")\n",
        "print(f\"Accuracy: {logreg_accuracy:.2f}\")\n",
        "print(f\"Precision: {logreg_precision:.2f}\")\n",
        "print(f\"Recall: {logreg_recall:.2f}\")\n",
        "print(f\"F1 Score: {logreg_f1:.2f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Calculate and print metrics for SVC\n",
        "svc_accuracy = accuracy_score(test_labels_numeric, svc_predictions)\n",
        "svc_precision = precision_score(test_labels_numeric, svc_predictions)\n",
        "svc_recall = recall_score(test_labels_numeric, svc_predictions)\n",
        "svc_f1 = f1_score(test_labels_numeric, svc_predictions)\n",
        "\n",
        "print(\"Metrics for SVC with CountVectorizer:\")\n",
        "print(f\"Accuracy: {svc_accuracy:.2f}\")\n",
        "print(f\"Precision: {svc_precision:.2f}\")\n",
        "print(f\"Recall: {svc_recall:.2f}\")\n",
        "print(f\"F1 Score: {svc_f1:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4779677d"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Optimize the hyperparameters of the chosen model and the vectorizer using techniques like Grid Search or Random Search to find the best configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da2458fb"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary modules and define the parameter grid for GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d69d2d8"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define a pipeline for Logistic Regression with CountVectorizer\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', LogisticRegression(solver='liblinear', max_iter=1000)) # Use 'liblinear' for smaller datasets and add max_iter\n",
        "])\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'vectorizer__max_features': [1000, 2000, 3000],\n",
        "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
        "    'classifier__C': [0.1, 1, 10]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4c33691"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate and fit GridSearchCV to find the best parameters and evaluate the best model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8b2765d"
      },
      "source": [
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=['accuracy', 'precision', 'recall', 'f1'], refit='f1')\n",
        "\n",
        "# Fit GridSearchCV to the cleaned training data\n",
        "grid_search.fit(train_messages_cleaned, train_labels_numeric)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "print(\"\\nBest cross-validation F1 score:\")\n",
        "print(f\"{grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Evaluate the best model on the cleaned test data\n",
        "best_model = grid_search.best_estimator_\n",
        "test_predictions = best_model.predict(test_messages_cleaned)\n",
        "\n",
        "# Calculate and print evaluation metrics on the test set\n",
        "test_accuracy = accuracy_score(test_labels_numeric, test_predictions)\n",
        "test_precision = precision_score(test_labels_numeric, test_predictions)\n",
        "test_recall = recall_score(test_labels_numeric, test_predictions)\n",
        "test_f1 = f1_score(test_labels_numeric, test_predictions)\n",
        "\n",
        "print(\"\\nMetrics on the cleaned test set with the best model:\")\n",
        "print(f\"Accuracy: {test_accuracy:.2f}\")\n",
        "print(f\"Precision: {test_precision:.2f}\")\n",
        "print(f\"Recall: {test_recall:.2f}\")\n",
        "print(f\"F1 Score: {test_f1:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd54d65c"
      },
      "source": [
        "## Cross-validation\n",
        "\n",
        "### Subtask:\n",
        "Implement cross-validation to get a more robust estimate of the model's performance and ensure it generalizes well to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06ab9b92"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement cross-validation using the best model found in the previous step to get a more robust performance estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "056842d9"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# Use cross_validate with the best model and cleaned training data\n",
        "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "cv_results = cross_validate(best_model, train_messages_cleaned, train_labels_numeric, cv=5, scoring=scoring)\n",
        "\n",
        "# Print the mean and standard deviation of the cross-validation scores\n",
        "print(\"Cross-validation results:\")\n",
        "for metric in scoring:\n",
        "    mean_score = cv_results[f'test_{metric}'].mean()\n",
        "    std_score = cv_results[f'test_{metric}'].std()\n",
        "    print(f\"Mean {metric.capitalize()}: {mean_score:.2f} (+/- {std_score:.2f})\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "fcc_sms_text_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}